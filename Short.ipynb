{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab85509",
   "metadata": {},
   "source": [
    "## Team name: Group4\n",
    "### Simone Bellini, Student ID 567572, Kaggle name: Simone Bellini\n",
    "### Miguel Baptista, Student ID 566511 Kaggle name: Miguel Baptista\n",
    "### Laura Rodriguez, Student ID 566591 Kaggle name: lrodriguez2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c62c2-b8de-4a5e-a36f-e8696688d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from datetime import date\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import warnings\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42081773",
   "metadata": {},
   "source": [
    "-Table description:\n",
    "--\n",
    "-beer_train.csv - the training set\n",
    "-beer_test.csv - the test set\n",
    "-sampleSubmission.csv - a sample submission file in the correct format\n",
    "-id_store_sku.csv - id - store_id - sku_name mapping\n",
    "-sku_features.csv - SKU Features\n",
    "\n",
    "-Feature description:\n",
    "--\n",
    "-Date - calendar date\n",
    "-ts_id - time-series identification, each of them describe sales of one SKU at one Store\n",
    "-isPromo - how many types of promotion were active on this day. 0 = no promotions, 1 = a single promotion type (for example, -price discount), 2 = two promotion types simultaneously (for example, both price discount and special placement)-\n",
    "-Sales - actual sales, target\n",
    "-id - identifier of rows in the submission file (the same order as in the test set). This is the key column Kaggle uses to match the test labels and your predictions. Order matters\n",
    "-Store - identifier of the stores (supermarket)\n",
    "-SKU - Stock Keeping Unit. Identifier of the product\n",
    "-Segment - price segment of the product\n",
    "-Pack - package type\n",
    "-Product - type of the drink\n",
    "-Brand - product's brand\n",
    "-Volume - package size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f9781-fc21-4b68-9931-de9f9d44b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_train = pd.read_csv(\"data/beer_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4683fd-3435-4a3c-952e-989c532d8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_test = pd.read_csv(\"data/beer_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0385ed-2deb-4a8a-93e5-e224b3b8b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission = pd.read_csv(\"data/sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd67042-5980-463a-b407-86726028bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_store_sku = pd.read_csv(\"data/id_store_sku.csv\")\n",
    "id_store_sku.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a5f5c-b35f-416c-82cf-b7bc7afd82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_features = pd.read_csv(\"data/sku_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb734b-6a9c-4f04-a7db-587783d6805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(id_store_sku, sku_features, on=\"SKU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10314f-daf8-465e-9e56-63f15169fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.merge(beer_train, result, on=\"ts_id\")\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ce0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_test = pd.merge(beer_test, result, on=\"ts_id\")\n",
    "final_result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce2057",
   "metadata": {},
   "source": [
    "# Visualization (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c3868",
   "metadata": {},
   "source": [
    "There are 267 stores and 18 SKU in id_store_sku\n",
    "There are 19 SKU in sku_features\n",
    "There are 256 stores, 18 SKU, in the final join\n",
    "\n",
    "That means that we don't have sales for 11 stores, and a beer is not sold anywhere\n",
    "\n",
    "\n",
    "There are 1944 ts_id in id_store_sku\n",
    "There are 1891 ts_id in beer_train and beer_test\n",
    "We have 173 missing ts_id in beer_train and beer_test\n",
    "We have 120 missing ts_id in id_store_sku\n",
    "\n",
    "That means that for 53 ts_ids we don't have sales, for 120 we don't have sales either correspondance with beer/store\n",
    "\n",
    "\n",
    "Date\n",
    "For the train, we have 731 data points(days) for each ts_id\n",
    "For the test, we have 102 data points(days) for each ts_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7833fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = final_result.copy(deep = True)\n",
    "fr['Date'] = pd.to_datetime(final_result['Date'])\n",
    "\n",
    "fr = fr[fr['Brand']==\"Don\"]\n",
    "fr\n",
    "res = fr.groupby(fr['Date'].dt.date).mean()  \n",
    "res\n",
    "plot_cols = ['Sales','isPromo']#'isPromo'\n",
    "plot_features = res[plot_cols]\n",
    "_ = plot_features.plot(subplots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "ts_id0 = final_result[final_result['ts_id']==0]\n",
    "date_time = pd.to_datetime(ts_id0.pop('Date'), format='%Y-%m-%d')\n",
    "plot_cols = ['Sales','isPromo','Segment']\n",
    "plot_features = ts_id0[plot_cols]\n",
    "plot_features.index = date_time\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d9631",
   "metadata": {},
   "source": [
    "### Effectiveness of is_Promo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2af7f",
   "metadata": {},
   "source": [
    "PACK Can, PET, Bottle\n",
    "Can : Seasonality around mid-september/begin june 2x (down) and new year/may (up)\n",
    "PET : september(down)\n",
    "Bottle : new year (up)\n",
    "\n",
    "SEGMENT Premium, MainStream, High Margin\n",
    "Premium : Seasonality new year (up), september/june (down)\n",
    "MainStream: maybe little trend up, september/june (down) HIGH CORR WITH ISPROMO !!\n",
    "High Margin: only 51 ts_id, 0 since OCT 2015, HIGH CORR WITH ISPROMO !!\n",
    "\n",
    "PRODUCT Lager, Energy-drink, Non-alco, Other\n",
    "Lager: september/june (down) new year (up)\n",
    "Energy-drink: 0 since OCT 2015, GOOD CORR WITH ISPROMO !!\n",
    "Non-alco: REAALLY VARIABLE,  new year (up), up in summer IS PROMO USELESS !!\n",
    "Other: VERY HIGH CORR WITH ISPROMO !!\n",
    "\n",
    "VOLUME 0.45, 0.5, 1.5\n",
    "0.45: september/june (down), new year (up)\n",
    "0.5: september/june (down), new year (up)\n",
    "1.5: GOOD CORR WITH ISPROMO !!\n",
    "\n",
    "STORE: TOO MANY\n",
    "\n",
    "BRAND Carlsberg Flash Tuborg Zatecky Gus Baltika №3 Baltika №7 Baltika №9 Baltika Cooler Baltika Draught Don Firmennoe Priazovskoye\n",
    "Carlsberg : HIGH CORR WITH ISPROMO !!\n",
    "Flash : GOOD CORR WITH ISPROMO !!\n",
    "Holsten : no data\n",
    "Tuborg : GOOD CORR WITH ISPROMO !!\n",
    "Zatecky Gus : GOOD CORR WITH ISPROMO !!\n",
    "Baltika №3 : GOOD CORR WITH ISPROMO !!\n",
    "Baltika №7 : GOOD CORR WITH ISPROMO !!\n",
    "Baltika №9 : september/june (down)\n",
    "Baltika Cooler : HIGH CORR WITH ISPROMO !!\n",
    "Baltika Draught : HIGH CORR WITH ISPROMO !!\n",
    "Don : GOOD CORR WITH ISPROMO !!\n",
    "Firmennoe : GOOD CORR WITH ISPROMO !!\n",
    "Priazovskoye : random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07169efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "final_result ['ProductPromoEffectiveness'] = np.where(final_result['Product'] == (\"Other\" or \"Energy-drink\"), 1, 0)\n",
    "final_result ['VolumePromoEffectiveness'] = np.where(final_result['Volume'] == 1.5, 1, 0)\n",
    "final_result ['BrandPromoEffectiveness'] = np.where(final_result['Brand'] != (\"Baltika №9\" or \"Priazovskoye\"), 1, 0)\n",
    "\n",
    "#TEST\n",
    "final_result_test ['ProductPromoEffectiveness'] = np.where(final_result_test['Product'] == (\"Other\" or \"Energy-drink\"), 1, 0)\n",
    "final_result_test ['VolumePromoEffectiveness'] = np.where(final_result_test['Volume'] == 1.5, 1, 0)\n",
    "final_result_test ['BrandPromoEffectiveness'] = np.where(final_result_test['Brand'] != (\"Baltika №9\" or \"Priazovskoye\"), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0859718",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b767ad2",
   "metadata": {},
   "source": [
    "### Transform into regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "final_result['Year'] = pd.to_datetime(final_result['Date']).dt.year\n",
    "final_result['Month'] = pd.to_datetime(final_result['Date']).dt.month\n",
    "final_result['Day'] = pd.to_datetime(final_result['Date']).dt.day\n",
    "final_result['Dayofweek'] = pd.to_datetime(final_result['Date']).dt.dayofweek\n",
    "final_result['Week'] = pd.to_datetime(final_result['Date']).dt.isocalendar().week\n",
    "final_result['Is_month_start'] = pd.to_datetime(final_result['Date']).dt.is_month_start\n",
    "# #Start of month you get paid and can spend money on beers\n",
    "final_result['Is_month_end'] = pd.to_datetime(final_result['Date']).dt.is_month_end\n",
    "final_result['Is_year_start'] = pd.to_datetime(final_result['Date']).dt.is_year_start\n",
    "final_result['Is_year_end'] = pd.to_datetime(final_result['Date']).dt.is_year_end\n",
    "final_result['Is_weekend'] = np.where(final_result['Dayofweek'].isin([5,6]),1,0)\n",
    "final_result['Is_weekday'] = np.where(final_result['Dayofweek'].isin([0,1,2,3,4]),1,0)\n",
    "\n",
    "#SEASON\n",
    "def get_season(date_time):\n",
    "    # dummy leap year to include leap days(year-02-29) in our range\n",
    "    leap_year = 2000\n",
    "    seasons = [('winter', (date(leap_year, 1, 1), date(leap_year, 3, 20))),\n",
    "               ('spring', (date(leap_year, 3, 21), date(leap_year, 6, 20))),\n",
    "               ('summer', (date(leap_year, 6, 21), date(leap_year, 9, 22))),\n",
    "               ('autumn', (date(leap_year, 9, 23), date(leap_year, 12, 20))),\n",
    "               ('winter', (date(leap_year, 12, 21), date(leap_year, 12, 31)))]\n",
    "\n",
    "    if isinstance(date_time, datetime):\n",
    "        date_time = date_time.date()\n",
    "    # we don't really care about the actual year so replace it with our dummy leap_year\n",
    "    date_time = date_time.replace(year=leap_year)\n",
    "    # return season our date falls in.\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= date_time <= end)\n",
    "\n",
    "\n",
    "# The apply method calls a function on each row\n",
    "final_result['Season'] = pd.to_datetime(final_result['Date']).apply(get_season)\n",
    "\n",
    "#BIRTHDAYS    \n",
    "val = [1150200, 1137254, 1251302, 1177421, 1185114, 1242651, 1361300, 1347711, 1278152, 1312967, 1198601, 1257869]\n",
    "#https://knoema.com/infographics/fjsoxbc/which-are-the-busiest-months-for-births\n",
    "\n",
    "final_result['Birthdays'] = final_result['Month']\n",
    "for x in range (1,13):\n",
    "    final_result.loc[final_result['Month'] == x, 'Birthdays'] = val[x-1]/sum(val)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9d81d",
   "metadata": {},
   "source": [
    "### IsPromo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019df66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['ts_promo'] = final_result['ts_id'].astype(str) + final_result['isPromo'].astype(str)\n",
    "final_result['store_promo'] = final_result['Store'].astype(str) + final_result['isPromo'].astype(str)\n",
    "final_result['segment_promo'] = final_result['Segment'].astype(str) + final_result['isPromo'].astype(str)\n",
    "final_result['brand_promo'] = final_result['Brand'].astype(str) + final_result['isPromo'].astype(str)\n",
    "final_result['sku_promo'] = final_result['SKU'].astype(str) + final_result['isPromo'].astype(str)\n",
    "\n",
    "final_result['dom_promo'] = final_result['Day'].astype(str) + final_result['isPromo'].astype(str)\n",
    "final_result['dow_promo'] = final_result['Dayofweek'].astype(str) + final_result['Dayofweek'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f85b52-3eb0-4d4e-b98e-ac3c097959cf",
   "metadata": {},
   "source": [
    "#### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3f8a4-4d2e-4069-912e-5af248767bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "holidays2015 = pd.read_csv(\"data/holidays2015.csv\")\n",
    "holidays2016 = pd.read_csv(\"data/holidays2016.csv\")\n",
    "holidays2017 = pd.read_csv(\"data/holidays2017.csv\")\n",
    "\n",
    "hol_nam1 = pd.DataFrame(data=holidays2015[\"Name\"].unique())\n",
    "\n",
    "hol_nam2 = pd.DataFrame(data=holidays2016[\"Name\"].unique())\n",
    "\n",
    "hol_nam3 = pd.DataFrame(data=holidays2017[\"Name\"].unique())\n",
    "\n",
    "date_time1 = pd.to_datetime(holidays2015.pop('Date'), format='%d-%b')\n",
    "holidays2015['Date'] = date_time1\n",
    "holidays2015[\"Date\"]=holidays2015[\"Date\"].dt.strftime(\"2015-%m-%d\")\n",
    "\n",
    "date_time2 = pd.to_datetime(holidays2016.pop('Date'), format='%d-%b')\n",
    "holidays2016['Date'] = date_time2\n",
    "holidays2016[\"Date\"]=holidays2016[\"Date\"].dt.strftime(\"2016-%m-%d\")\n",
    "\n",
    "date_time3 = pd.to_datetime(holidays2017.pop('Date'), format='%d-%b')\n",
    "holidays2017['Date'] = date_time3\n",
    "holidays2017[\"Date\"]=holidays2017[\"Date\"].dt.strftime(\"2017-%m-%d\")\n",
    "\n",
    "result_holidays = pd.merge(\n",
    "    hol_nam1,\n",
    "    hol_nam2,\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "result_holidays = pd.merge(\n",
    "    result_holidays,\n",
    "    hol_nam3,\n",
    "    how=\"outer\"\n",
    ")\n",
    "names_holidays = list(result_holidays[0])\n",
    "\n",
    "matrix = np.zeros((len(final_result),len(names_holidays)))\n",
    "holidays_df = pd.DataFrame(matrix,\n",
    "                   columns=names_holidays)\n",
    "\n",
    "final_result = final_result.join(holidays_df)\n",
    "\n",
    "aux = 0\n",
    "for index, row in final_result.iterrows():\n",
    "    if row['Year']==2015:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2015['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result.at[index, holidays2015['Name'][ind]]=1\n",
    "                final_result.at[index,\"Holiday\"] = holidays2015['Name'][ind]\n",
    "        else:\n",
    "            final_result.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "    if row['Year']==2016:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2016['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result.at[index,holidays2016['Name'][ind]]=1\n",
    "                final_result.at[index, \"Holiday\"]=holidays2016['Name'][ind]\n",
    "        else:\n",
    "            final_result.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "    if row['Year']==2017:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2017['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result.at[index , holidays2017['Name'][ind]]=1\n",
    "                final_result.at[index, \"Holiday\"]=holidays2017['Name'][ind]\n",
    "        else:\n",
    "            final_result.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb67f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = final_result.drop(holidays_df,axis=1)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST SET\n",
    "final_result_test['Year'] = pd.to_datetime(final_result_test['Date']).dt.year\n",
    "final_result_test['Month'] = pd.to_datetime(final_result_test['Date']).dt.month\n",
    "final_result_test['Day'] = pd.to_datetime(final_result_test['Date']).dt.day\n",
    "final_result_test['Dayofweek'] = pd.to_datetime(final_result_test['Date']).dt.dayofweek\n",
    "final_result_test['Week'] = pd.to_datetime(final_result_test['Date']).dt.isocalendar().week\n",
    "final_result_test['Is_month_start'] = pd.to_datetime(final_result_test['Date']).dt.is_month_start\n",
    "# #Start of month you get paid and can spend money on beers\n",
    "final_result_test['Is_month_end'] = pd.to_datetime(final_result_test['Date']).dt.is_month_end\n",
    "final_result_test['Is_year_start'] = pd.to_datetime(final_result_test['Date']).dt.is_year_start\n",
    "final_result_test['Is_year_end'] = pd.to_datetime(final_result_test['Date']).dt.is_year_end\n",
    "final_result_test['Is_weekend'] = np.where(final_result_test['Dayofweek'].isin([5,6]),1,0)\n",
    "final_result_test['Is_weekday'] = np.where(final_result_test['Dayofweek'].isin([0,1,2,3,4]),1,0)\n",
    "\n",
    "#SEASON\n",
    "# The apply method calls a function on each row\n",
    "final_result_test['Season'] = pd.to_datetime(final_result_test['Date']).apply(get_season)\n",
    "\n",
    "#BIRTHDAYS    \n",
    "val = [1150200, 1137254, 1251302, 1177421, 1185114, 1242651, 1361300, 1347711, 1278152, 1312967, 1198601, 1257869]\n",
    "#https://knoema.com/infographics/fjsoxbc/which-are-the-busiest-months-for-births\n",
    "\n",
    "final_result_test['Birthdays'] = final_result_test['Month']\n",
    "for x in range (1,13):\n",
    "    final_result_test.loc[final_result_test['Month'] == x, 'Birthdays'] = val[x-1]/sum(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19facc",
   "metadata": {},
   "source": [
    "### IsPromo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_test['ts_promo'] = final_result_test['ts_id'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "final_result_test['store_promo'] = final_result_test['Store'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "final_result_test['segment_promo'] = final_result_test['Segment'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "final_result_test['brand_promo'] = final_result_test['Brand'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "final_result_test['sku_promo'] = final_result_test['SKU'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "\n",
    "final_result_test['dom_promo'] = final_result_test['Day'].astype(str) + final_result_test['isPromo'].astype(str)\n",
    "final_result_test['dow_promo'] = final_result_test['Dayofweek'].astype(str) + final_result_test['isPromo'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09c29b-86e4-4492-8af1-1f77c5365541",
   "metadata": {},
   "source": [
    "#### Holidays test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa5c8b-2469-451c-81eb-ea54571d0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(final_result_test),len(names_holidays)))\n",
    "holidays_df = pd.DataFrame(matrix,\n",
    "                   columns=names_holidays)\n",
    "\n",
    "aux = 0\n",
    "for index, row in final_result_test.iterrows():\n",
    "    if row['Year']==2015:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2015['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result_test.at[index, holidays2015['Name'][ind]]=1\n",
    "                final_result_test.at[index,\"Holiday\"] = holidays2015['Name'][ind]\n",
    "        else:\n",
    "            final_result_test.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "    if row['Year']==2016:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2016['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result_test.at[index,holidays2016['Name'][ind]]=1\n",
    "                final_result_test.at[index, \"Holiday\"]=holidays2016['Name'][ind]\n",
    "        else:\n",
    "            final_result_test.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "    if row['Year']==2017:\n",
    "        indexes = [i for i, e in enumerate(list(holidays2017['Date'])) if e == row['Date']]\n",
    "        if indexes :\n",
    "            for ind in indexes:\n",
    "                final_result_test.at[index , holidays2017['Name'][ind]]=1\n",
    "                final_result_test.at[index, \"Holiday\"]=holidays2017['Name'][ind]\n",
    "        else:\n",
    "            final_result_test.at[index, \"Holiday\"]=\"Not Holiday\"\n",
    "\n",
    "final_result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158aedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_test = final_result_test.drop(['Lailat al-Qadr','June Solstice','Eid al-Fitr','Day of Knowledge','Eid al-Adha','Muharram','September Equinox'],axis=1)\n",
    "final_result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653ee53",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find if there are duplicates\n",
    "final_result = pd.DataFrame.drop_duplicates(final_result)# Nothing happens\n",
    "beer_test = pd.DataFrame.drop_duplicates(beer_test)# Nothing happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find if there are nan\n",
    "final_result.isnull().sum().sum() #No nan found\n",
    "beer_test.isnull().sum().sum() #No nan found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7469b",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267edd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"train_preprocessed.csv\", index=False)\n",
    "final_result_test.to_csv(\"test_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdab658",
   "metadata": {},
   "source": [
    "## Algorithm 1: avg (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train_preprocessed.csv\",dtype={'Store': object,'ts_promo': object,'store_promo': object,'dom_promo': object,'dow_promo': object})\n",
    "trainData['Date'] = pd.to_datetime(trainData['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e529f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"test_preprocessed.csv\",dtype={'Store': object,'ts_promo': object,'store_promo': object,'dom_promo': object,'dow_promo': object})\n",
    "testData['Date'] = pd.to_datetime(testData['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in trainData.columns:\n",
    "    col_type = trainData[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        trainData[c] = trainData[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in testData.columns:\n",
    "    col_type = testData[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        testData[c] = testData[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea830bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainData['Date'].is_monotonic_increasing)\n",
    "print(testData['Date'].is_monotonic_increasing)\n",
    "trainData = trainData.sort_values(by=['Date'])\n",
    "testData = testData.sort_values(by=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12356e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainData.drop(['Sales','Date'], axis=1)#Date\n",
    "y = trainData['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_categ =[]\n",
    "for c in X.columns:\n",
    "    col_type = X[c].dtype\n",
    "    if col_type.name == 'category':\n",
    "        var_categ.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(\n",
    "    boosting_type = 'dart',\n",
    "    learning_rate = 0.35878911553612336, \n",
    "    max_depth = 1,\n",
    "    metric='rmse',\n",
    "    n_estimators = 524,\n",
    "    num_leaves = 21,\n",
    "    subsample_for_bin = 199000,\n",
    "    colsample_bytree = 0.8,\n",
    "    subsample = 1.0,\n",
    "    silent=True\n",
    "    )\n",
    "testX = testData.drop(['id','Date'], axis=1)\n",
    "if 'Sales' in testX.columns:\n",
    "    testX = testX.drop(['Sales'], axis=1)\n",
    "\n",
    "#Now all data\n",
    "lgbmfit = lgbm.fit(X,y)\n",
    "lgbm_prediction = lgbm.predict(testX)\n",
    "\n",
    "testData['Sales'] = lgbm_prediction\n",
    "testData = testData.sort_values(by=['id'])\n",
    "testData[['id','Sales']].to_csv(\"LGBMsubmission.csv\", index=False)\n",
    "lgbm_prediction = copy.copy(testData['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostRegressor(\n",
    "    n_estimators=64,\n",
    "    learning_rate=0.14038578250228806,\n",
    "    depth=8,\n",
    "    bagging_temperature=0.1075748346436656,\n",
    "    border_count = 213,\n",
    "    l2_leaf_reg = 12,\n",
    "    random_strength = 5.03421359307061e-09,\n",
    "    cat_features=var_categ,\n",
    "    thread_count=-1,\n",
    "    silent=True\n",
    ")\n",
    "testX = testData.drop(['id','Date'], axis=1)\n",
    "if 'Sales' in testX.columns:\n",
    "    testX = testX.drop(['Sales'], axis=1)\n",
    "\n",
    "#Now all data\n",
    "catboost.fit(X,y)\n",
    "catboost_prediction = catboost.predict(testX)\n",
    "testData['Sales'] = catboost_prediction\n",
    "testData = testData.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54686bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_prediction.to_numpy()\n",
    "avg_prediction = np.mean([lgbm_prediction, catboost_prediction], axis=0)\n",
    "testData['Sales'] = avg_prediction\n",
    "testData = testData.sort_values(by=['id'])\n",
    "testData[['id','Sales']].to_csv(\"avg1.csv\", index=False)\n",
    "testData[['id','Sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a43f96",
   "metadata": {},
   "source": [
    "# Algorithm 2: avg (other submission chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostRegressor(\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.427912392471475789,\n",
    "    depth=8,\n",
    "    bagging_temperature=0.1080748346321565,\n",
    "    border_count = 210,\n",
    "    l2_leaf_reg = 10,\n",
    "    random_strength = 5.03391369357891e-09,\n",
    "    cat_features=var_categ,\n",
    "    thread_count=-1,\n",
    "    silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86156204",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testData.drop(['id','Date'], axis=1)\n",
    "if 'Sales' in testX.columns:\n",
    "    testX = testX.drop(['Sales'], axis=1)\n",
    "\n",
    "#Now all data\n",
    "catboost.fit(X,y)\n",
    "catboost_prediction2 = catboost.predict(testX)\n",
    "\n",
    "testData['Sales'] = catboost_prediction2\n",
    "testDataSorted = testData.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeafa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(\n",
    "    boosting_type = 'dart',\n",
    "    learning_rate = 0.039691509923932582, \n",
    "    max_depth = 10,\n",
    "    metric='rmse',\n",
    "    n_estimators = 330,\n",
    "    num_leaves = 21,\n",
    "    subsample_for_bin = 201000,\n",
    "    colsample_bytree = 0.8,\n",
    "    subsample = 1.0,\n",
    "    silent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testData.drop(['id','Date'], axis=1)\n",
    "if 'Sales' in testX.columns:\n",
    "    testX = testX.drop(['Sales'], axis=1)\n",
    "\n",
    "#Now all data\n",
    "lgbm.fit(X,y)\n",
    "lgbm_prediction2 = lgbm.predict(testX)\n",
    "\n",
    "testData['Sales'] = lgbm_prediction2\n",
    "testData = testData.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prediction = np.mean([lgbm_prediction, catboost_prediction,lgbm_prediction2, catboost_prediction2], axis=0)\n",
    "testData['Sales'] = avg_prediction\n",
    "testData = testData.sort_values(by=['id'])\n",
    "testData[['id','Sales']].to_csv(\"avg2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
